{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as pp\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "import ast\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from scipy.stats import sem\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading users info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ui = pd.read_excel('user_info.xlsx')\n",
    "print(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the scraped data (from past 6 months) and show the numbers of posts (=tweets+quote_tweets+replies_to_self) for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for username in ui['username']:\n",
    "    df[username] = pd.read_excel('data/' + username + '.xlsx')\n",
    "    print(username,df[username].shape[0],'posts')\n",
    "    # keep only up to 100 (randomly sampled) posts per user\n",
    "    df[username] = df[username].sample(frac=1,random_state=0)\n",
    "    df[username] = df[username].head(100)\n",
    "    # some basic text processing\n",
    "    df[username]['content'] = df[username]['content'].str.lower()\n",
    "    df[username]['content'] = df[username]['content'].str.replace('\\n',' ')\n",
    "    df[username]['content'] = df[username]['content'].str.replace('&amp;',' ')\n",
    "    df[username]['content'] = df[username]['content'].str.replace('&lt;',' ')\n",
    "    df[username]['content'] = df[username]['content'].str.replace('&gt;',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various functions to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what we define as \"punctuation\" here, i.e., symbols that need to be padded with whitespaces\n",
    "p = string.punctuation\n",
    "# do not remove hyphens \n",
    "p = p.replace('-','')\n",
    "# do not remove apostrophes\n",
    "p = p.replace('\\'','')\n",
    "punctuation_re = '([' + p + '])'\n",
    "\n",
    "def process_text(text,keep_punctuation=False,lemmatize=True):\n",
    "    # remove irrelevant tokens\n",
    "    pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "    text = pp.clean(text)\n",
    "    # remove \"punctuation\"\n",
    "    if not keep_punctuation:\n",
    "        text = re.sub(punctuation_re,' ',text)\n",
    "    # keep it but add spaces around the symbols\n",
    "    else:\n",
    "        text = re.sub(punctuation_re, r' \\1 ',text)\n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        doc = nlp(text)\n",
    "        sent = [word.lemma_ for word in doc]\n",
    "        text = \" \".join(sent)\n",
    "    return text\n",
    "\n",
    "def count_ngrams(text,n=1):\n",
    "    text = text.split()\n",
    "    if n==1: ngrams = Counter(text)\n",
    "    else: ngrams = Counter(zip(*[islice(text,i,None) for i in range(n)]))\n",
    "    return ngrams\n",
    "\n",
    "def get_top_n_grams(text,top=10,ngram=1):\n",
    "    ngrams = count_ngrams(text,ngram)\n",
    "    sorted_ngrams = ngrams.most_common()\n",
    "    total = sum(c for ngram, c in sorted_ngrams)\n",
    "    if ngram>1: sorted_ngrams = [(' '.join(ng[0]),ng[1],100*ng[1]/total) for ng in sorted_ngrams]\n",
    "    else: sorted_ngrams = [(ng[0],ng[1],100*ng[1]/total) for ng in sorted_ngrams]\n",
    "    return sorted_ngrams[:top]\n",
    "\n",
    "def get_keyness(corpus1,corpus2,n1,n2,ngram=1):\n",
    "    # corpus1 is a list of N strings, each string representing a concatenation of the tweets from an author \n",
    "    # belonging to the group this is the corpus of\n",
    "    # create a list of all the words from authors in corpus1\n",
    "    words = ' '.join(corpus1)\n",
    "    # technically, these are the unigrams\n",
    "    words = words.split()\n",
    "    # transform them into ngrams if needed\n",
    "    if ngram>1:\n",
    "        words = list(zip(*[words[i:] for i in range(ngram)]))\n",
    "    keyness = {}\n",
    "    # the list of unique ngrams (to save time when searching the texts contain them)\n",
    "    unique_words = list(set(words))\n",
    "    \n",
    "    # compute ngram frequencies in corpus1 and corpus2\n",
    "    word_freq_1 = []\n",
    "    for author in corpus1:\n",
    "        author_words = author.split()\n",
    "        if ngram>1: author_words = list(zip(*[author_words[i:] for i in range(ngram)]))\n",
    "        word_freq_1.append(Counter(author_words))      \n",
    "    word_freq_2 = []\n",
    "    for author in corpus2:\n",
    "        author_words = author.split()\n",
    "        if ngram>1: author_words = list(zip(*[author_words[i:] for i in range(ngram)]))\n",
    "        freq = Counter(author_words)\n",
    "        word_freq_2.append(freq)    \n",
    "    \n",
    "    for word in unique_words:\n",
    "        o1 = 0\n",
    "        f1 = 0\n",
    "        for freq in word_freq_1:\n",
    "            o1 = o1 + int(freq[word]>0)\n",
    "            f1 = f1 + freq[word]\n",
    "        o2 = 0\n",
    "        for freq in word_freq_2:\n",
    "            o2 = o2 + int(freq[word]>0)\n",
    "        # check number of tweets instead of number of authors for which it appears?\n",
    "        e1 = n1*(o1+o2)/(n1+n2)\n",
    "        e2 = n2*(o1+o2)/(n1+n2)\n",
    "        p1 = o1*np.log(o1/e1) if o1>0 else 0\n",
    "        p2 = o2*np.log(o2/e2) if o2>0 else 0\n",
    "        k = 2*(p1+p2)\n",
    "        #if word == 'thanks': print(o1,o2,np.log(o1/e1),np.log(o2/e2),n1,n2,e1,e2,p1,p2)\n",
    "        keyness[word] = (k,f1)\n",
    "    return keyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute keyness of words for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus1 = {}\n",
    "n1 = {}\n",
    "ng = 1\n",
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'disciplinaryGroup'\n",
    "\n",
    "for group in ui[grouping].unique():\n",
    "    corpus1[group] = []\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        text = df[username].content.str.cat(sep=' ')\n",
    "        #text = [process_text(t,keep_punctuation=False,lemmatize=False) for t in text]\n",
    "        text = process_text(text,keep_punctuation=False,lemmatize=False)\n",
    "        #corpus1[group] = corpus1[group] + text\n",
    "        corpus1[group].append(text)\n",
    "    n1[group]=len(usernames)    \n",
    "        \n",
    "keyness = {}\n",
    "for discipline in corpus1.keys():\n",
    "    n2 = 0\n",
    "    corpus2 = []\n",
    "    for other_discipline in corpus1.keys():\n",
    "        if discipline==other_discipline: continue\n",
    "        corpus2 = corpus2 + corpus1[other_discipline]\n",
    "        n2 = n2 + n1[other_discipline]\n",
    "    keyness[discipline] = get_keyness(corpus1[discipline],corpus2,n1[discipline],n2,ngram=ng)\n",
    "    \n",
    "for group in ui[grouping].unique():\n",
    "    print(\"==== \" + group + \" ====\")\n",
    "    dfk = pd.DataFrame(keyness[group].items(),columns=['word', 'keyness'])\n",
    "    dfk = dfk.sort_values(by=['keyness'],ascending=False)\n",
    "    print(dfk.head(10).to_string(index=False),'\\n')\n",
    "    #dfk.to_excel('results/keyness_' + str(ng) + '-gram_' + group + '.xlsx', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all content (text without urls, emojis, mentions, smileys, hashtags) of all users within one discipline and export it to a txt file + export all tweets of each users to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'pureApplied'\n",
    "for group in ui[grouping].unique():\n",
    "    tot_text = ''\n",
    "    for username in ui[ui[grouping]==group].username.tolist():\n",
    "        pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "        user_text = pp.clean(df[username].content.str.cat(sep=' '))\n",
    "        tot_text = tot_text + user_text\n",
    "        txt_file = open('results/' + username + '.txt','w')\n",
    "        txt_file.write(user_text)\n",
    "        txt_file.close()\n",
    "    txt_file = open('results/' + group + '.txt','w')\n",
    "    txt_file.write(tot_text)\n",
    "    txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and display unigrams/bigrams/trigrams for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'pureApplied'\n",
    "\n",
    "for group in ui[grouping].unique():\n",
    "    text = ''\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        text = text + df[username].content.str.cat(sep=' ')\n",
    "    processed_text = process_text(text,keep_punctuation=False,lemmatize=True)\n",
    "    unigrams = get_top_n_grams(processed_text,50,1)\n",
    "    processed_text = process_text(text,keep_punctuation=True,lemmatize=True)\n",
    "    bigrams = get_top_n_grams(processed_text,50,2)\n",
    "    trigrams = get_top_n_grams(processed_text,50,3)\n",
    "    stats = pd.DataFrame(unigrams,columns = ['unigram','#uni','%uni'])\n",
    "    stats['bigrams'],stats['#bi'],stats['%bi'] = zip(*bigrams)\n",
    "    stats['trigrams'],stats['#tri'],stats['%tri'] = zip(*trigrams)\n",
    "    print('==== \\033[1m' + group.upper() + '\\033[0m ====')\n",
    "    with pd.option_context('display.float_format','{:0.2f}'.format,'expand_frame_repr', False):\n",
    "        print(stats)\n",
    "    print('\\n\\n')\n",
    "    #stats.to_excel('results/ngrams_' + group + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the engagement rate for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'disciplinaryGroup'\n",
    "\n",
    "for group in ui[grouping].unique():\n",
    "    engagements = []\n",
    "    tot_followers = 0\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        num_followers = df[username]['user.followersCount'].values[0]\n",
    "        tot_followers = tot_followers + num_followers\n",
    "        avg_engagement = df[username]['engagementRate'].mean()\n",
    "        engagements.append(avg_engagement)\n",
    "        print(username + ' {:.2f}'.format(avg_engagement) + '% avg eng and ' + str(num_followers) + ' followers')\n",
    "        \n",
    "    print('\\033[1m' + group.upper() + ' = {:.2f}'.format(np.mean(engagements)) + \n",
    "          '% ± {:.2f}'.format(sem(engagements)) + '\\033[0m')\n",
    "    print('Avg num of followers: {:.1f}'.format(tot_followers/len(usernames)))\n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average length of tweets (in characters) per user and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'disciplinaryGroup'\n",
    "\n",
    "for group in ui[grouping].unique():\n",
    "    tot_length_discipline = []\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "        tot_length_user = 0\n",
    "        for tweet in df[username]['content']:\n",
    "            cleaned_tweet = pp.clean(tweet)\n",
    "            tot_length_user = tot_length_user + len(cleaned_tweet)\n",
    "        print(username,'{:.1f}'.format(tot_length_user/len(df[username]['content'])))\n",
    "        tot_length_discipline.append(tot_length_user/len(df[username]['content']))\n",
    "    print('\\033[1m' + group.upper(),'{:.1f}'.format(np.mean(tot_length_discipline)),\n",
    "          ' ± {:.2f}'.format(sem(tot_length_discipline)) + '\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many single tweets/threads/quote per user and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'pureApplied'\n",
    "\n",
    "for group in ui[grouping].unique():\n",
    "    num_quote_tweets_discipline = []\n",
    "    num_single_tweets_discipline = []\n",
    "    num_threads_discipline = []\n",
    "    avg_threads_discipline = []\n",
    "    df_results = pd.DataFrame(columns = ['-','#single','#thread','avg len thread','#quotes'])\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        num_quote_tweets_user = sum(df[username]['quotedTweet.url']!='None')\n",
    "        num_quote_tweets_discipline.append(num_quote_tweets_user)\n",
    "        counter = df[username]['conversationId'].value_counts()\n",
    "        num_single_tweets_user = sum(counter==1)\n",
    "        num_single_tweets_discipline.append(num_single_tweets_user)\n",
    "        num_threads_user = sum(counter>1)\n",
    "        num_threads_discipline.append(num_threads_user)\n",
    "        avg_threads_user = sum(counter[counter>1])/len(counter[counter>1]) if sum(counter>1) else 0\n",
    "        avg_threads_discipline.append(avg_threads_user)\n",
    "        row = {'-':username,'#single':num_single_tweets_user,'#thread':num_threads_user,\n",
    "               'avg len thread':'{:.1f}'.format(avg_threads_user),'#quotes':num_quote_tweets_user}\n",
    "        df_results = df_results.append(row, ignore_index = True)    \n",
    "    row = {'-':group.upper(),\n",
    "           '#single':'{:.1f}'.format(sum(num_single_tweets_discipline)/len(num_single_tweets_discipline)) +\n",
    "           '± {:.1f}'.format(sem(num_single_tweets_discipline)),\n",
    "           '#thread':'{:.1f}'.format(sum(num_threads_discipline)/len(num_threads_discipline)) +\n",
    "           '± {:.1f}'.format(sem(num_threads_discipline)),\n",
    "           'avg len thread':'{:.1f}'.format(sum(avg_threads_discipline)/len(avg_threads_discipline)) +\n",
    "           '± {:.1f}'.format(sem(avg_threads_discipline)),\n",
    "           '#quotes':'{:.1f}'.format(sum(num_quote_tweets_discipline)/len(num_quote_tweets_discipline)) +\n",
    "           '± {:.1f}'.format(sem(num_quote_tweets_discipline))}\n",
    "    df_results = df_results.append(row, ignore_index = True)\n",
    "    #df_results.to_excel('results/tweet-type_' + group + '.xlsx',header=True,index=None)\n",
    "    print(df_results,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many emojis/hashtags/urls/photos/videos/gifs per user and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'disciplinaryGroup'\n",
    "\n",
    "for group in ui[grouping].unique():\n",
    "    emojis_discipline = []\n",
    "    hashtags_discipline = []\n",
    "    mentions_discipline = []\n",
    "    urls_discipline = []\n",
    "    photos_discipline = []\n",
    "    videos_discipline = []\n",
    "    gifs_discipline = []\n",
    "    \n",
    "    df_results = pd.DataFrame(columns = ['-','#emojis','#hashtags','#urls','#mentions','#photos','#videos','#gifs'])\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        emojis_user = 0\n",
    "        hashtags_user = 0\n",
    "        urls_user = 0\n",
    "        mentions_user = 0\n",
    "        photos_user = 0\n",
    "        videos_user = 0\n",
    "        gifs_user = 0\n",
    "        \n",
    "        for tweet in df[username]['content']:\n",
    "            tokens = pp.tokenize(tweet)\n",
    "            emojis_user = emojis_user + tokens.count('$EMOJI')\n",
    "            hashtags_user = hashtags_user + tokens.count('$HASHTAG')\n",
    "            mentions_user = mentions_user + tokens.count('$MENTION')\n",
    "        emojis_discipline.append(emojis_user)\n",
    "        hashtags_discipline.append(hashtags_user)\n",
    "        mentions_discipline.append(mentions_user)\n",
    "        \n",
    "        for ols in df[username]['outlinks']:\n",
    "            if ols=='None': continue\n",
    "            not_twitter = []\n",
    "            if 'twitter.com' in ols: continue\n",
    "            ols = ast.literal_eval(ols)\n",
    "            urls_user = urls_user + len(ols)\n",
    "        urls_discipline.append(urls_user)\n",
    "        \n",
    "        for medias in df[username]['media']:\n",
    "            if medias=='None': continue\n",
    "            medias = ast.literal_eval(medias)\n",
    "            for media in medias:\n",
    "                photos_user = photos_user + 1 if 'snscrape.modules.twitter.Photo' in media['_type'] else photos_user\n",
    "                videos_user = videos_user + 1 if 'snscrape.modules.twitter.Video' in media['_type'] else videos_user\n",
    "                gifs_user = gifs_user + 1 if 'snscrape.modules.twitter.Gif' in media['_type'] else gifs_user\n",
    "        photos_discipline.append(photos_user)\n",
    "        videos_discipline.append(videos_user)\n",
    "        gifs_discipline.append(gifs_user)\n",
    "        \n",
    "        row = {'-':username,'#emojis':emojis_user,'#hashtags':hashtags_user,\n",
    "               '#urls':urls_user,'#mentions':mentions_user,'#photos':photos_user,'#videos':videos_user,\n",
    "               '#gifs':gifs_user}\n",
    "        df_results = df_results.append(row, ignore_index = True)\n",
    "    row = {'-':group.upper(),\n",
    "           '#emojis':'{:.1f}'.format(sum(emojis_discipline)/len(emojis_discipline)) +\n",
    "           '± {:.1f}'.format(sem(emojis_discipline)),\n",
    "           '#hashtags':'{:.1f}'.format(sum(hashtags_discipline)/len(hashtags_discipline)) +\n",
    "           '± {:.1f}'.format(sem(hashtags_discipline)),\n",
    "           '#urls':'{:.1f}'.format(sum(urls_discipline)/len(urls_discipline)) +\n",
    "           '± {:.1f}'.format(sem(urls_discipline)),\n",
    "           '#mentions':'{:.1f}'.format(sum(mentions_discipline)/len(mentions_discipline)) +\n",
    "           '± {:.1f}'.format(sem(mentions_discipline)),\n",
    "           '#photos':'{:.1f}'.format(sum(photos_discipline)/len(photos_discipline)) +\n",
    "           '± {:.1f}'.format(sem(photos_discipline)),\n",
    "           '#videos':'{:.1f}'.format(sum(videos_discipline)/len(videos_discipline)) +\n",
    "           '± {:.1f}'.format(sem(videos_discipline)),\n",
    "           '#gifs':'{:.1f}'.format(sum(gifs_discipline)/len(gifs_discipline)) +\n",
    "           '± {:.1f}'.format(sem(gifs_discipline))}\n",
    "    df_results = df_results.append(row, ignore_index = True)\n",
    "    #df_results.to_excel('results/modalities_' + group + '.xlsx',header=True,index=None)\n",
    "    print(df_results,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the frequency (per 1000 words) of engagement types for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_types = ['boosters','hedges','reader','self_mention','directives']\n",
    "for engagement_type in engagement_types:\n",
    "    print(\"======\" + engagement_type + \"======\")\n",
    "    with open('engagement_search_items/' + engagement_type + '.txt') as file:\n",
    "        engagement_items = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    #disciplinaryGroup,softHard,pureApplied\n",
    "    grouping = 'disciplinaryGroup'\n",
    "    for group in ui[grouping].unique():\n",
    "        tot_text = ''\n",
    "        for username in ui[ui[grouping]==group].username.tolist():\n",
    "            pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "            user_text = pp.clean(df[username].content.str.cat(sep=' '))\n",
    "            tot_text = tot_text + ' ' + user_text\n",
    "        \n",
    "        tot_text = tot_text.split()\n",
    "        occurences = []\n",
    "        for word in engagement_items:\n",
    "            count = tot_text.count(word)\n",
    "            occurences.append(count)\n",
    "        print(group,sum(occurences)/len(tot_text)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the frequency (per 1000 words) of engagement types for each user and the averages per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'disciplinaryGroup'\n",
    "\n",
    "engagement_types = ['boosters','hedges','reader','self_mention','directives']\n",
    "engagement_items = {key:[] for key in engagement_types} \n",
    "for engagement_type in engagement_types:\n",
    "    with open('engagement_search_items/' + engagement_type + '.txt') as file:\n",
    "        engagement_items[engagement_type] = [line.rstrip('\\n') for line in file]\n",
    "                        \n",
    "for group in ui[grouping].unique():\n",
    "    discipline_count =  {key:[] for key in engagement_types}\n",
    "    df_results = pd.DataFrame(columns = ['-','#boosters','#hedges','#reader','#self_mention','#directives'])\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    for username in usernames:\n",
    "        user_count = dict.fromkeys(engagement_types, 0)        \n",
    "        pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "        user_text = pp.clean(df[username].content.str.cat(sep=' '))\n",
    "        user_text = user_text.split()\n",
    "        \n",
    "        for engagement_type in engagement_types:\n",
    "            for word in engagement_items[engagement_type]:\n",
    "                user_count[engagement_type] = user_count[engagement_type] + user_text.count(word)\n",
    "            user_count[engagement_type] = 1000*user_count[engagement_type]/len(user_text)\n",
    "            discipline_count[engagement_type].append(user_count[engagement_type])\n",
    "        \n",
    "        row = {'-':username,'#boosters':user_count['boosters'],'#hedges':user_count['hedges'],\n",
    "               '#reader':user_count['reader'],'#self_mention':user_count['self_mention'],'#directives':user_count['directives']}\n",
    "        df_results = df_results.append(row, ignore_index = True)\n",
    "    \n",
    "    row = {'-':group.upper(),\n",
    "               '#boosters':'{:.1f}'.format(sum(discipline_count['boosters'])/len(discipline_count['boosters'])) +\n",
    "               '± {:.1f}'.format(sem(discipline_count['boosters'])),\n",
    "               '#hedges':'{:.1f}'.format(sum(discipline_count['hedges'])/len(discipline_count['hedges'])) +\n",
    "               '± {:.1f}'.format(sem(discipline_count['hedges'])),\n",
    "               '#reader':'{:.1f}'.format(sum(discipline_count['reader'])/len(discipline_count['reader'])) +\n",
    "               '± {:.1f}'.format(sem(discipline_count['reader'])),\n",
    "               '#self_mention':'{:.1f}'.format(sum(discipline_count['self_mention'])/len(discipline_count['self_mention'])) +\n",
    "               '± {:.1f}'.format(sem(discipline_count['self_mention'])),\n",
    "               '#directives':'{:.1f}'.format(sum(discipline_count['directives'])/len(discipline_count['directives'])) +\n",
    "               '± {:.1f}'.format(sem(discipline_count['directives']))}\n",
    "    df_results = df_results.append(row, ignore_index = True)\n",
    "    #df_results.to_excel('results/engagement_' + group + '.xlsx',header=True,index=None)\n",
    "    print(df_results,'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many 'words' per user (without counting url, emoji, mentions, smileys, hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'disciplinaryGroup'\n",
    "\n",
    "tot_all = 0\n",
    "for group in ui[grouping].unique():\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    tot = 0\n",
    "    for username in usernames:\n",
    "        user_count = dict.fromkeys(engagement_types, 0)        \n",
    "        pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "        user_text = pp.clean(df[username].content.str.cat(sep=' '))\n",
    "        user_text = process_text(user_text,keep_punctuation=True,lemmatize=False)\n",
    "        user_text = user_text.split()\n",
    "        tot = tot + len(user_text)\n",
    "    tot_all = tot_all + tot\n",
    "    print(group,tot)\n",
    "print('ALL =', tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many question (marks) for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disciplinaryGroup,softHard,pureApplied\n",
    "s = \"!\"\n",
    "grouping = 'disciplinaryGroup'\n",
    "for group in ui[grouping].unique():\n",
    "    tot_text = ''\n",
    "    tokens_group = []\n",
    "    for username in ui[ui[grouping]==group].username.tolist():\n",
    "        pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "        user_text = pp.clean(df[username].content.str.cat(sep=' '))\n",
    "        tokens_group = tokens_group + user_text.split()\n",
    "        tot_text = tot_text + user_text\n",
    "        print(username,str(round(user_text.count(s)/len(user_text.split())*1000,2)))\n",
    "    print(group,str(round(tot_text.count(s)/len(tokens_group)*1000,2)))\n",
    "    print(\"========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sentiment (vader-compound) for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#disciplinaryGroup,softHard,pureApplied\n",
    "grouping = 'pureApplied'\n",
    "for group in ui[grouping].unique():\n",
    "    usernames = ui[ui[grouping]==group].username.tolist()\n",
    "    compounds = [] \n",
    "    pp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.MENTION, pp.OPT.SMILEY, pp.OPT.HASHTAG)\n",
    "    for username in usernames:\n",
    "        for tweet in df[username]['content']:\n",
    "            cleaned_tweet = pp.clean(tweet)\n",
    "            vs = analyzer.polarity_scores(cleaned_tweet)\n",
    "            compounds.append(vs['compound'])\n",
    "    mean = np.mean(compounds)\n",
    "    print(group + \" \" + str(round(mean,4)) + \"±\" + str(round(sem(compounds),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Some unfinished code that tests the measurement of p-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import t as ttable\n",
    "\n",
    "a = [38,16,0,2,21,51,66,19,5,7]\n",
    "b = [44,85,62,3,11,0,0,22,9,2]\n",
    "c = [3,0,0,8,15,0,1,0,20,1]\n",
    "\n",
    "sea = sem(a)\n",
    "seb = sem(b)\n",
    "sec = sem(c)\n",
    "\n",
    "print(sea,seb,sec)\n",
    "\n",
    "\n",
    "result=ttest_ind(b,c,equal_var=True)\n",
    "if result.pvalue <= 0.02:\n",
    "    print('+++')\n",
    "elif result.pvalue > 0.1:\n",
    "    print('+')\n",
    "else:\n",
    "    print('++')\n",
    "print(result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
